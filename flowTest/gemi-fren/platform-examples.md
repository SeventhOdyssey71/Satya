# Platform Examples & Real-World Use Cases

## Complete User Journey Examples

### First-Time User: Model Upload Journey
```
SCENARIO: New user wants to upload their first PyTorch model

USER: "Hi, I'm new here. How do I upload my AI model?"

AGENT: "Welcome to Satya! I'll guide you through uploading your first AI model step by step.

        First, let me understand what you're working with:
        ‚Ä¢ What type of model is it? (e.g., image classification, NLP, etc.)
        ‚Ä¢ What format is your model file? (.pt, .h5, .onnx, etc.)  
        ‚Ä¢ Roughly what size is the file?

        This helps me provide the most relevant guidance for your specific situation."

USER: "It's a ResNet50 for image classification, PyTorch .pt file, about 98MB"

AGENT: "Perfect! ResNet50 is a popular computer vision model, and .pt files are fully supported.
        98MB is a good size - not too large for verification issues.

        Here's what will happen:
        1. **Upload** (2-3 minutes): Your file gets encrypted and stored on Walrus
        2. **TEE Verification** (6-12 hours): Secure analysis for quality and security  
        3. **Results**: You'll get a quality score (0-100) and can list on marketplace

        The upload costs about 0.1 SUI in gas fees, and verification is free.

        Ready to start? I can redirect you to the upload page with computer vision 
        pre-selected, or would you like more details about any step first?"

USER: "Let's start the upload"

AGENT: "Excellent! I'm redirecting you to the upload page with these settings:
        ‚úì Category: Computer Vision
        ‚úì File type: PyTorch (.pt)
        ‚úì Estimated verification: 8 hours

        Once you upload, I'll track the progress and notify you when verification completes.
        You can ask me 'check my model status' anytime for updates."

[User uploads successfully]

AGENT: "Great! Your ResNet50 model is uploaded successfully.
        
        üìÅ File: resnet50_v2.pt (98.2 MB)  
        üîë Model ID: 0x7a8b9c...
        ‚è≥ Status: Queued for verification (position #23, ~6 hours)
        
        I'll notify you when verification completes. In the meantime, you can:
        ‚Ä¢ Browse the marketplace to see similar models
        ‚Ä¢ Learn about pricing strategies for computer vision models
        ‚Ä¢ Explore other platform features
        
        Would you like me to show you some high-performing CV models for reference?"
```

### Experienced User: Bulk Operations
```
SCENARIO: Power user managing multiple models and marketplace optimization

USER: "I have 8 verified models that aren't selling well. Help me optimize them."

AGENT: "I'll analyze your 8 models and provide optimization recommendations.
        Fetching your model performance data...

        üìä **Performance Analysis for Your 8 Models:**

        **Underperforming Models (3):**
        ‚Ä¢ VGG19_custom (0 sales, 60 days) - Price: 2.1 SUI vs market avg 0.8 SUI
        ‚Ä¢ BERT_sentiment (2 sales, 45 days) - Quality: 72/100 vs top models 85+  
        ‚Ä¢ MobileNet_v3 (1 sale, 30 days) - Missing description & examples

        **Moderate Performers (3):**
        ‚Ä¢ ResNet152 (5 sales, 60 days) - Good quality (89) but high price
        ‚Ä¢ GPT2_finetune (8 sales, 20 days) - Popular category, decent pricing
        ‚Ä¢ YOLOv5_custom (4 sales, 40 days) - Good performance, needs better keywords

        **Strong Performers (2):**
        ‚Ä¢ EfficientNet_B7 (15 sales, 30 days) - Excellent quality & pricing
        ‚Ä¢ Transformer_custom (22 sales, 45 days) - Market leader in category

        **Optimization Recommendations:**

        üéØ **Immediate Actions (High Impact):**
        1. **Reduce VGG19 price**: 2.1 SUI ‚Üí 0.9 SUI (163% increase in similar models)
        2. **Enhance MobileNet listing**: Add description, code examples, use cases  
        3. **Add keywords to YOLOv5**: 'object detection', 'real-time', 'mobile'

        üí° **Strategic Improvements:**
        1. **Retrain BERT model**: Current 72 quality vs 85+ for top sellers
        2. **Bundle complementary models**: Package ResNet+MobileNet for mobile deployment
        3. **Create model variants**: Quantized versions of high-performers

        Would you like me to execute the pricing changes now, or would you prefer 
        to review the detailed analysis first?"

USER: "Execute the pricing changes and show me the bundling options"

AGENT: "Executing pricing updates...

        ‚úÖ VGG19_custom: Updated from 2.1 ‚Üí 0.9 SUI
        ‚úÖ ResNet152: Updated from 1.8 ‚Üí 1.3 SUI  
        ‚úÖ YOLOv5_custom: Updated from 1.2 ‚Üí 1.0 SUI
        
        **Model Bundling Opportunities:**

        üéÅ **Bundle 1: Mobile AI Kit**
        ‚Ä¢ MobileNet_v3 + EfficientNet_B7 + Quantized versions
        ‚Ä¢ Target: Mobile app developers
        ‚Ä¢ Suggested price: 3.2 SUI (vs 4.1 individual) - 22% bundle discount
        ‚Ä¢ Market demand: High (152 searches/week for 'mobile AI')

        üéÅ **Bundle 2: Complete Vision Pipeline**  
        ‚Ä¢ YOLOv5_custom + ResNet152 + Post-processing tools
        ‚Ä¢ Target: Computer vision system builders
        ‚Ä¢ Suggested price: 2.8 SUI (vs 3.3 individual) - 15% bundle discount
        ‚Ä¢ Market demand: Medium (89 searches/week for 'vision pipeline')

        Creating these bundles will:
        ‚Ä¢ Increase average order value by 40-60%
        ‚Ä¢ Improve customer discovery through bundle keywords
        ‚Ä¢ Position you as a comprehensive solution provider

        Shall I create these bundle listings with the recommended pricing?"
```

### Problem Resolution: Complex Technical Issue
```
SCENARIO: User experiencing verification failures with custom architecture

USER: "My model has been failing verification for 3 days. This is really frustrating!"

AGENT: "I understand your frustration - 3 days of failed verifications is definitely not normal. 
        Let me investigate this immediately.

        Analyzing your recent verification attempts...

        üîç **Issue Analysis for Model: custom_transformer_v2.pt**

        **Failed Attempts (3):**
        ‚Ä¢ Attempt 1 (Nov 19): Failed at structure validation - 'Unknown layer type: CustomAttention'
        ‚Ä¢ Attempt 2 (Nov 20): Failed at security scan - 'Suspicious code in model metadata' 
        ‚Ä¢ Attempt 3 (Nov 21): Failed at compatibility check - 'PyTorch version mismatch'

        **Root Cause Identified:**
        Your model uses custom layers that aren't in the TEE verification whitelist, 
        plus there's metadata from a newer PyTorch version than our current support.

        **Immediate Solutions:**

        üõ†Ô∏è **Option 1: Model Compatibility Fix (Recommended)**
        I'll guide you through converting custom layers to standard equivalents:
        ‚Ä¢ CustomAttention ‚Üí torch.nn.MultiheadAttention  
        ‚Ä¢ Export with PyTorch 1.13 (compatible version)
        ‚Ä¢ Clean metadata to remove version conflicts

        üõ†Ô∏è **Option 2: Whitelist Request (2-3 day process)**
        Submit your custom layers for security review and whitelist approval:
        ‚Ä¢ Provide layer source code for audit
        ‚Ä¢ Wait for security team approval
        ‚Ä¢ Re-verify with approved custom layers

        üõ†Ô∏è **Option 3: Architecture Refactor**
        Redesign using only standard PyTorch operations:
        ‚Ä¢ Replace custom layers with equivalent standard ones
        ‚Ä¢ May require some performance tuning
        ‚Ä¢ Guaranteed compatibility with current system

        **Compensation for Your Time:**
        Given the repeated failures, I'm crediting your account 0.5 SUI for gas fees 
        and prioritizing your next verification (queue position #1).

        Which approach would you prefer? I can start with Option 1 and have you 
        uploading a compatible version within an hour."

USER: "Let's try option 1, but I need those custom layers for performance"

AGENT: "I understand the performance concern. Let me show you how to maintain performance 
        while using standard layers.

        **Performance-Preserving Conversion Guide:**

        üîÑ **CustomAttention ‚Üí MultiheadAttention Mapping:**
        ```python
        # Your current custom layer (simplified)
        class CustomAttention(nn.Module):
            def __init__(self, dim, heads=8, dropout=0.1):
                # ... custom implementation
        
        # Standard equivalent with same performance  
        standard_attention = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=heads, 
            dropout=dropout,
            batch_first=True  # Match your custom behavior
        )
        ```

        **Verification Results from Similar Conversions:**
        ‚Ä¢ 97% performance retention on average
        ‚Ä¢ Some users see 1-3% improvement due to optimized standard ops
        ‚Ä¢ Zero compatibility issues with marketplace or deployment

        **Step-by-Step Conversion Process:**
        1. I'll provide complete conversion code for each custom layer
        2. You run conversion script (5 minutes)
        3. Quick local validation test (2 minutes)  
        4. Upload new compatible model (3 minutes)
        5. Priority verification (30 minutes vs 8 hours)

        **Performance Guarantee:**
        If converted model performs >5% worse than original, I'll help you get 
        custom layers whitelisted at no additional cost.

        Ready to start the conversion? I'll provide the exact code for your 
        CustomAttention layer first."
```

## Advanced Use Case Scenarios

### Enterprise Integration: API-Driven Workflows  
```
SCENARIO: Company integrating Satya into their ML pipeline

USER: "We need to integrate model uploads into our CI/CD pipeline. Can you help?"

AGENT: "Absolutely! I can help you design an automated ML model deployment pipeline 
        integrating with Satya. Let me understand your current setup first:

        **Current Pipeline Assessment:**
        ‚Ä¢ What CI/CD platform? (GitHub Actions, Jenkins, GitLab, etc.)
        ‚Ä¢ Model training frequency? (Daily, weekly, on-commit, etc.)
        ‚Ä¢ Quality gates? (Accuracy thresholds, performance requirements, etc.)
        ‚Ä¢ Deployment targets? (Production, staging, marketplace, etc.)

        Based on typical enterprise needs, here's a recommended architecture:"

USER: "GitHub Actions, weekly training runs, 85% accuracy minimum, deploy to both staging and marketplace"

AGENT: "Perfect! Here's a complete GitHub Actions integration for your weekly ML pipeline:

        **üèóÔ∏è Automated Pipeline Architecture:**

        ```yaml
        # .github/workflows/ml-pipeline.yml
        name: ML Model Pipeline
        on:
          schedule:
            - cron: '0 2 * * 1'  # Weekly Monday 2AM
          workflow_dispatch:
        
        jobs:
          train-and-deploy:
            runs-on: ubuntu-latest
            steps:
              - name: Train Model
                run: python train_model.py
                
              - name: Validate Quality
                run: |
                  accuracy=$(python evaluate_model.py)
                  if (( $(echo \"$accuracy < 0.85\" | bc -l) )); then
                    echo \"‚ùå Model accuracy $accuracy below 85% threshold\"
                    exit 1
                  fi
                  echo \"‚úÖ Model accuracy $accuracy meets requirements\"
                
              - name: Upload to Satya
                env:
                  SATYA_API_KEY: ${{ secrets.SATYA_API_KEY }}
                run: |
                  python scripts/satya_upload.py \\
                    --model \"models/production_model.pt\" \\
                    --category \"computer_vision\" \\
                    --auto-list \\
                    --price-strategy \"market_competitive\"
        ```

        **üìã Integration Components I'll Provide:**

        1. **Satya Upload Script (satya_upload.py):**
           ‚Ä¢ Handles file upload with retry logic
           ‚Ä¢ Monitors verification progress  
           ‚Ä¢ Auto-creates marketplace listing on success
           ‚Ä¢ Sends notifications to Slack/email

        2. **Quality Gate Integration:**
           ‚Ä¢ Pre-upload validation against your thresholds
           ‚Ä¢ Automatic rollback if verification fails
           ‚Ä¢ Performance benchmarking against previous versions

        3. **Monitoring Dashboard:**
           ‚Ä¢ Track all automated uploads
           ‚Ä¢ Verification success rates
           ‚Ä¢ Marketplace performance metrics
           ‚Ä¢ Alert integration for failures

        **üîí Security & Best Practices:**
        ‚Ä¢ API keys stored in GitHub secrets  
        ‚Ä¢ Model checksums for integrity verification
        ‚Ä¢ Automated testing in isolated environment
        ‚Ä¢ Rollback procedures for failed deployments

        **üìä Expected Benefits:**
        ‚Ä¢ 90% reduction in manual deployment effort
        ‚Ä¢ 24-hour faster time-to-market (vs manual process)  
        ‚Ä¢ Zero deployment errors from automation
        ‚Ä¢ Full audit trail for compliance

        Would you like me to generate the complete integration scripts, or shall we 
        start with a specific component first?"

USER: "Generate the upload script first, then we'll test it"

AGENT: "Here's your production-ready Satya upload script with enterprise features:

        **üìÅ satya_upload.py - Enterprise Integration Script:**

        ```python
        #!/usr/bin/env python3
        \"\"\"
        Satya Model Upload Script for CI/CD Integration
        Handles automated model upload, verification monitoring, and marketplace listing
        \"\"\"
        
        import os, sys, time, requests, json
        from pathlib import Path
        from typing import Optional, Dict, Any
        
        class SatyaUploader:
            def __init__(self, api_key: str, base_url: str = \"https://api.satya.ai\"):
                self.api_key = api_key
                self.base_url = base_url
                self.session = requests.Session()
                self.session.headers.update({
                    'Authorization': f'Bearer {api_key}',
                    'User-Agent': 'Satya-CI-CD-Integration/1.0'
                })
        
            def upload_model(self, model_path: Path, metadata: Dict[str, Any]) -> str:
                \"\"\"Upload model and return model_id\"\"\"
                print(f\"üöÄ Starting upload: {model_path.name} ({model_path.stat().st_size / 1024/1024:.1f}MB)\")
                
                with open(model_path, 'rb') as f:
                    response = self.session.post(
                        f'{self.base_url}/models/upload',
                        files={'model': f},
                        data=metadata
                    )
                
                if response.status_code != 200:
                    raise Exception(f'Upload failed: {response.text}')
                
                model_id = response.json()['model_id']
                print(f\"‚úÖ Upload complete. Model ID: {model_id[:16]}...\")
                return model_id
        
            def monitor_verification(self, model_id: str, timeout_hours: int = 12) -> Dict[str, Any]:
                \"\"\"Monitor verification progress with timeout\"\"\"
                print(f\"‚è≥ Monitoring verification for {model_id[:16]}...\")
                
                start_time = time.time()
                while time.time() - start_time < timeout_hours * 3600:
                    response = self.session.get(f'{self.base_url}/models/{model_id}/status')
                    status = response.json()
                    
                    print(f\"   Status: {status['state']} ({status.get('progress', 0)}%)\")
                    
                    if status['state'] == 'verified':
                        print(f\"‚úÖ Verification complete! Quality score: {status['quality_score']}/100\")
                        return status
                    elif status['state'] == 'failed':
                        raise Exception(f'Verification failed: {status.get(\"error\", \"Unknown error\")}')
                    
                    time.sleep(60)  # Check every minute
                
                raise TimeoutError(f'Verification timeout after {timeout_hours} hours')
        
            def create_listing(self, model_id: str, pricing_strategy: str = 'market_competitive') -> str:
                \"\"\"Create marketplace listing with intelligent pricing\"\"\"
                print(f\"üí∞ Creating marketplace listing with {pricing_strategy} pricing...\")
                
                listing_data = {
                    'model_id': model_id,
                    'pricing_strategy': pricing_strategy,
                    'auto_price': True
                }
                
                response = self.session.post(f'{self.base_url}/marketplace/listings', json=listing_data)
                
                if response.status_code != 200:
                    raise Exception(f'Listing creation failed: {response.text}')
                
                listing = response.json()
                print(f\"‚úÖ Listed on marketplace: {listing['listing_url']}\")
                print(f\"   Price: {listing['price']} SUI (based on {pricing_strategy})\")
                return listing['listing_id']
        
        def main():
            # Parse command line arguments
            import argparse
            parser = argparse.ArgumentParser(description='Upload model to Satya platform')
            parser.add_argument('--model', required=True, help='Path to model file')
            parser.add_argument('--category', required=True, help='Model category')
            parser.add_argument('--auto-list', action='store_true', help='Auto-create marketplace listing')
            parser.add_argument('--price-strategy', default='market_competitive', help='Pricing strategy')
            args = parser.parse_args()
            
            # Initialize uploader
            api_key = os.environ.get('SATYA_API_KEY')
            if not api_key:
                print(\"‚ùå SATYA_API_KEY environment variable required\")
                sys.exit(1)
            
            uploader = SatyaUploader(api_key)
            
            try:
                # Upload model
                model_path = Path(args.model)
                metadata = {
                    'category': args.category,
                    'name': model_path.stem,
                    'description': f'Automated upload from CI/CD pipeline',
                    'tags': ['automated', 'production', args.category]
                }
                
                model_id = uploader.upload_model(model_path, metadata)
                
                # Monitor verification  
                status = uploader.monitor_verification(model_id)
                
                # Create marketplace listing if requested
                if args.auto_list and status['quality_score'] >= 80:
                    listing_id = uploader.create_listing(model_id, args.price_strategy)
                    print(f\"üìà Marketplace listing created: {listing_id}\")
                elif args.auto_list:
                    print(f\"‚ö†Ô∏è  Quality score {status['quality_score']}/100 below 80, skipping marketplace listing\")
                
                print(f\"üéâ Pipeline complete! Model {model_id[:16]}... ready for production\")
                
            except Exception as e:
                print(f\"‚ùå Pipeline failed: {e}\")
                sys.exit(1)
        
        if __name__ == '__main__':
            main()
        ```

        **üß™ Testing Instructions:**

        1. **Setup API Key:**
           ```bash
           export SATYA_API_KEY=\"your-api-key-here\"
           ```

        2. **Test Upload (without listing):**
           ```bash
           python satya_upload.py \\
             --model \"test_model.pt\" \\
             --category \"computer_vision\"
           ```

        3. **Full Pipeline Test:**
           ```bash
           python satya_upload.py \\
             --model \"production_model.pt\" \\
             --category \"computer_vision\" \\
             --auto-list \\
             --price-strategy \"market_competitive\"
           ```

        **üìä Features Included:**
        ‚Ä¢ ‚úÖ Robust error handling with detailed logging
        ‚Ä¢ ‚úÖ Progress monitoring with timeout protection
        ‚Ä¢ ‚úÖ Intelligent marketplace pricing strategies  
        ‚Ä¢ ‚úÖ Quality gates (minimum score 80 for auto-listing)
        ‚Ä¢ ‚úÖ CI/CD friendly exit codes and output format
        ‚Ä¢ ‚úÖ Comprehensive metadata and tagging

        Test this with a sample model first, then we'll integrate it into your GitHub Actions workflow. 
        Let me know how the test goes!"
```